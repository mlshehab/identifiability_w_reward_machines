{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('policy.npy', 'rb') as f:\n",
    "    policy = np.load(f)\n",
    "    q = np.load(f)\n",
    "    v = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e565cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(policy[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677dafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(policy,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9709f3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete state: (5, 9, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class Discretizer:\n",
    "    def __init__(self, bins):\n",
    "        self.bins = bins\n",
    "        self.bin_limits = [\n",
    "            [-4.8, 4.8],          # Cart position\n",
    "            [-2.0, 2.0],          # Cart velocity\n",
    "            [-4.1887903e-01, 4.1887903e-01],          # Pole angle\n",
    "            [-2.0, 2.0]           # Pole angular velocity\n",
    "        ]\n",
    "        \n",
    "    def discretize(self, state):\n",
    "        state_adj = []\n",
    "        for i in range(len(state)):\n",
    "            state_adj.append(int(np.digitize(state[i], np.linspace(self.bin_limits[i][0], self.bin_limits[i][1], self.bins)) ))\n",
    "        return tuple(state_adj)\n",
    "\n",
    "    def get_num_states(self):\n",
    "        return (self.bins,) * len(self.bin_limits)\n",
    "\n",
    "# Example discretization\n",
    "discretizer = Discretizer(bins=10)\n",
    "state = [0, 1.9, 0.1, 1.9]\n",
    "discrete_state = discretizer.discretize(state)\n",
    "print(\"Discrete state:\", discrete_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DisDyn(env, bins = 10)\n",
    "print((  dd.continuous_to_discrete(state[0],0) , \\\n",
    "         dd.continuous_to_discrete(state[1],1) , \\\n",
    "         dd.continuous_to_discrete(state[2],2) , \\\n",
    "         dd.continuous_to_discrete(state[3],3)  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9a86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4cb0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744ee31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a8a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af50e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actions is 2\n",
      "The number of states is 810000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wt/49mgl7qj51zcv8cx_l3jq79r0000gp/T/ipykernel_2321/4080407932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mdiscretizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscretizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueIterationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscretizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/wt/49mgl7qj51zcv8cx_l3jq79r0000gp/T/ipykernel_2321/4080407932.py\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_discretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0;31m# get the index of the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mnext_state_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple_to_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wt/49mgl7qj51zcv8cx_l3jq79r0000gp/T/ipykernel_2321/686087652.py\u001b[0m in \u001b[0;36mdiscretize\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstate_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mstate_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(x, bins, right)\u001b[0m\n\u001b[1;32m   4930\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4931\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \"\"\"\n\u001b[0;32m-> 1350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'searchsorted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class ValueIterationAgent:\n",
    "    def __init__(self, env, discretizer, gamma=0.99, theta=1e-4):\n",
    "        self.env = env\n",
    "        self.discretizer = discretizer\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.num_actions = env.action_space.n\n",
    "        print(f\"The number of actions is {self.num_actions}\")\n",
    "#         self.num_states = discretizer.get_num_states()\n",
    "        self.num_states = self.discretizer.bins**len(self.discretizer.bin_limits)\n",
    "        print(f\"The number of states is {self.num_states}\")\n",
    "        self.V = np.zeros(self.num_states)\n",
    "        self.policy = np.zeros(self.num_states, dtype=int)\n",
    "        self.steps_beyond_terminated = 0\n",
    "        \n",
    "    def simulate(self,state, action):\n",
    "       \n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.env.force_mag if action == 1 else -self.env.force_mag\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.env.polemass_length * np.square(theta_dot) * sintheta\n",
    "        ) / self.env.total_mass\n",
    "        thetaacc = (self.env.gravity * sintheta - costheta * temp) / (\n",
    "            self.env.length\n",
    "            * (4.0 / 3.0 - self.env.masspole * np.square(costheta) / self.env.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.env.polemass_length * thetaacc * costheta / self.env.total_mass\n",
    "\n",
    "        if self.env.kinematics_integrator == \"euler\":\n",
    "            x = x + self.env.tau * x_dot\n",
    "            x_dot = x_dot + self.env.tau * xacc\n",
    "            theta = theta + self.env.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.env.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        state = np.array((x, x_dot, theta, theta_dot), dtype=np.float64)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.env.x_threshold\n",
    "            or x > self.env.x_threshold\n",
    "            or theta < -self.env.theta_threshold_radians\n",
    "            or theta > self.env.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward =  1.0\n",
    "      \n",
    "        else:\n",
    "#             if self.steps_beyond_terminated == 0:\n",
    "                \n",
    "            self.steps_beyond_terminated += 1\n",
    "\n",
    "            reward =  0.0\n",
    "\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return np.array(state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    \n",
    "    \n",
    "    def value_iteration(self):\n",
    "        it = 0            \n",
    "        while True:       \n",
    "            start_time = time.time()\n",
    "            it+=1         \n",
    "            delta = 0     \n",
    "            for s_i, s in enumerate(np.ndindex(*self.discretizer.get_num_states())):\n",
    "                if not s_i%10000:\n",
    "                    print(f\"i = {s_i}\")\n",
    "                    \n",
    "                action_values = np.zeros(self.num_actions)\n",
    "                v = self.V[s_i] \n",
    "\n",
    "                for a in range(self.num_actions):\n",
    "                    state = self.inverse_discretize(s)\n",
    "                    next_state, reward, done, _,_ = self.simulate(state, a)\n",
    "                    next_state = self.discretizer.discretize(next_state)\n",
    "                    # get the index of the next state \n",
    "                    next_state_i = self.tuple_to_state(next_state)\n",
    "                    action_values[a] = reward + (0 if done else self.gamma * self.V[next_state_i])\n",
    "#                 print(f\"State is {state}\")\n",
    "                self.V[s_i] = np.max(action_values)\n",
    "                self.policy[s_i] = np.argmax(action_values)\n",
    "                delta = max(delta, abs(v - self.V[s_i]))\n",
    "            end_time = time.time()\n",
    "            print(f\"it: {it} -- Time: {end_time - start_time:.2f} sec - ||delta||: {delta:.7f}\")\n",
    "            \n",
    "            if not it%5:\n",
    "                self.test_policy()\n",
    "                \n",
    "            if delta < self.theta:\n",
    "                break   \n",
    "                \n",
    "    def test_policy(self):\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            state, reward, done, _,_ = env.step(action)\n",
    "            total_reward += reward\n",
    "        #     env.render()\n",
    "\n",
    "        print(\"Tested the policy. Total Reward:\", total_reward)\n",
    "            \n",
    "    def tuple_to_state(self, t):\n",
    "\n",
    "        s1,s2,s3,s4 = t\n",
    "       \n",
    "        return s4 + s3*self.discretizer.bins + s2*self.discretizer.bins**2 + s1*self.discretizer.bins**3\n",
    "    \n",
    "    def inverse_discretize(self, discrete_state):\n",
    "        state = []\n",
    "        for i, val in enumerate(discrete_state):\n",
    "            state.append((val + 0.5) * (self.discretizer.bin_limits[i][1] - self.discretizer.bin_limits[i][0]) / self.discretizer.bins + self.discretizer.bin_limits[i][0])\n",
    "        return np.array(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        discrete_state = self.discretizer.discretize(state)\n",
    "        return self.policy[self.tuple_to_state(discrete_state)]\n",
    "\n",
    "# Example usage\n",
    "env = gym.make('CartPole-v1')\n",
    "discretizer = Discretizer(bins=20)\n",
    "agent = ValueIterationAgent(env, discretizer)\n",
    "agent.value_iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 15\n",
    "bin_limit = 4\n",
    "a = (bins,) * 4\n",
    "i = 0\n",
    "for s in np.ndindex(*a):\n",
    "    i+=1\n",
    "print(i)\n",
    "#     for i, val in enumerate(s):\n",
    "    \n",
    "#         print(f\"s is {s}, i is {i} and val is {val}\")\n",
    "for t in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# for i in range(100):\n",
    "#     state, _ = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "#         action = agent.get_action(state)\n",
    "#         state, reward, done, _,_ = env.step(action)\n",
    "#         total_reward += reward\n",
    "#     #     env.render()\n",
    "\n",
    "#     print(\"Total Reward:\", total_reward)\n",
    "# # env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time \n",
    "# import scipy\n",
    "from scipy.special import softmax, logsumexp\n",
    "# softmax(1)\n",
    "class DisDyn(object):\n",
    "\n",
    "    def __init__(self, env, bins):\n",
    "\n",
    "        self.env  = env\n",
    "        self.bins = bins\n",
    "        self.lb_v = -2.0\n",
    "        self.ub_v = 2.0\n",
    "        self.n_obs_states = self.env.observation_space.shape[0]\n",
    "        self.lb       = self.env.observation_space.low\n",
    "        self.lb[1]    = self.lb_v    # since the bounds on the velocities are defined to be inf\n",
    "        self.lb[-1]   = self.lb_v \n",
    "        self.ub       = self.env.observation_space.high\n",
    "        self.ub[1]    = self.ub_v\n",
    "        self.ub[-1]   = self.ub_v \n",
    "\n",
    "        self.n_states  = self.bins**self.n_obs_states\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        self.seen_dict = {}\n",
    "        self.c2et2s_dict = {}\n",
    "        self.simulated_dict = {}\n",
    "\n",
    "        self.state_to_tuple_dict = {}\n",
    "        self.tuple_to_state_dict = {}\n",
    "        self.d2c_d = {}\n",
    "        \n",
    "\n",
    "    def tuple_to_state(self, t):\n",
    "        if t in self.tuple_to_state_dict:\n",
    "            return self.tuple_to_state_dict[t]\n",
    "        \n",
    "        else:\n",
    "            s1,s2,s3,s4 = t\n",
    "            self.tuple_to_state_dict[t] = s4 + s3*self.bins + s2*self.bins**2 + s1*self.bins**3\n",
    "\n",
    "        return s4 + s3*self.bins + s2*self.bins**2 + s1*self.bins**3\n",
    "\n",
    "    def state_to_tuple(self,s):\n",
    "        if s in self.state_to_tuple_dict:\n",
    "            return self.state_to_tuple_dict[s]\n",
    "        else:\n",
    "            s1 = s // self.bins**3\n",
    "            rem = s % self.bins**3\n",
    "            \n",
    "            s2 = rem // self.bins**2\n",
    "            \n",
    "            rem2 = rem % self.bins**2\n",
    "            \n",
    "            s3 = rem2 // self.bins\n",
    "            s4 = rem2 % self.bins\n",
    "\n",
    "            self.state_to_tuple_dict[s] = (s1,s2,s3,s4)\n",
    "\n",
    "        return (s1,s2,s3,s4)\n",
    "\n",
    "\n",
    "    def discrete_to_continuous(self,s_d, i):\n",
    "        # i is the observation number \n",
    "        bin_width = (self.ub[i] - self.lb[i])/self.bins\n",
    "    \n",
    "        return self.lb[i] + (s_d + 0.5) * bin_width\n",
    "    \n",
    "    def continuous_to_discrete(self, s_c, i):\n",
    "        # i is the observation number\n",
    "        bin_width = (self.ub[i] - self.lb[i])/self.bins\n",
    "        if s_c <= self.lb[i]:\n",
    "            return 0\n",
    "        if s_c >= self.ub[i]:\n",
    "            return self.bins - 1\n",
    "        \n",
    "        return int((s_c - self.lb[i]) // bin_width)   \n",
    "\n",
    "  \n",
    "    def state_to_tuple_then_d2c(self, state):\n",
    "        if state in self.seen_dict:\n",
    "            return self.seen_dict[state]\n",
    "        else:\n",
    "            (s1,s2,s3,s4) = self.state_to_tuple(state)\n",
    "\n",
    "            # make them continuous\n",
    "            state_c = (self.discrete_to_continuous(s1,0),\\\n",
    "                        self.discrete_to_continuous(s2,1),\\\n",
    "                        self.discrete_to_continuous(s3,2),\\\n",
    "                        self.discrete_to_continuous(s4,3))\n",
    "            \n",
    "            self.seen_dict[state] = state_c\n",
    "            \n",
    "        return state_c\n",
    "\n",
    "    def c2d_then_tuple_to_state(self,next_state_c):\n",
    "       \n",
    "        next_state_tuple = (self.continuous_to_discrete(next_state_c[0],0) , \\\n",
    "                                        self.continuous_to_discrete(next_state_c[1],1) , \\\n",
    "                                        self.continuous_to_discrete(next_state_c[2],2) , \\\n",
    "                                        self.continuous_to_discrete(next_state_c[3],3) )\n",
    "                    \n",
    "        next_state = self.tuple_to_state(next_state_tuple)\n",
    "       \n",
    "        return next_state\n",
    "    \n",
    "    def simulate(self, state, action):\n",
    "        if state in self.simulated_dict:\n",
    "            if action in self.simulated_dict[state]:\n",
    "                return self.simulated_dict[state][action]\n",
    "        \n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.env.force_mag if action == 1 else -self.env.force_mag\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)      \n",
    "\n",
    "        temp = (\n",
    "            force + self.env.polemass_length * np.square(theta_dot) * sintheta\n",
    "        ) / self.env.total_mass\n",
    "        thetaacc = (self.env.gravity * sintheta - costheta * temp) / (\n",
    "            self.env.length\n",
    "            * (4.0 / 3.0 - self.env.masspole * np.square(costheta) / self.env.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.env.polemass_length * thetaacc * costheta / self.env.total_mass\n",
    "\n",
    "        if self.env.kinematics_integrator == \"euler\":\n",
    "            x = x + self.env.tau * x_dot\n",
    "            x_dot = x_dot + self.env.tau * xacc\n",
    "            theta = theta + self.env.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.env.tau * thetaacc\n",
    "\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.env.x_threshold\n",
    "            or x > self.env.x_threshold\n",
    "            or theta < -self.env.theta_threshold_radians\n",
    "            or theta > self.env.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        self.simulated_dict[state] = {}\n",
    "        self.simulated_dict[state][action] = np.array((x, x_dot, theta, theta_dot), dtype=np.float64) , reward , terminated\n",
    "\n",
    "        return  np.array((x, x_dot, theta, theta_dot), dtype=np.float64) , reward , terminated\n",
    "\n",
    "    def test_policy(self,policy, bellman):\n",
    "        if bellman == 'soft':\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            eps_len = 0\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                state_tuple = (self.continuous_to_discrete(state[0],0) , \\\n",
    "                            self.continuous_to_discrete(state[1],1) , \\\n",
    "                            self.continuous_to_discrete(state[2],2) , \\\n",
    "                            self.continuous_to_discrete(state[3],3) )\n",
    "                            \n",
    "                # this is the deterministic next state\n",
    "                state_disc = self.tuple_to_state(state_tuple)\n",
    "                p = policy[state_disc]\n",
    "\n",
    "                action = np.random.choice(np.array([0,1]), p = p)\n",
    "                next_state, reward , done, _,_ = self.env.step(action)\n",
    "                \n",
    "                state = next_state \n",
    "                           \n",
    "                eps_len +=1\n",
    "\n",
    "            print(f\"The {bellman} policy lasted for {eps_len} episodes.\") \n",
    "\n",
    "        else:\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            eps_len = 0\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                state_tuple = (self.continuous_to_discrete(state[0],0) , \\\n",
    "                            self.continuous_to_discrete(state[1],1) , \\\n",
    "                            self.continuous_to_discrete(state[2],2) , \\\n",
    "                            self.continuous_to_discrete(state[3],3) )\n",
    "                            \n",
    "                # this is the deterministic next state\n",
    "                state_disc = self.tuple_to_state(state_tuple)\n",
    "                action = policy[state_disc]\n",
    "\n",
    "              \n",
    "                next_state, reward , done, _,_ = self.env.step(action)\n",
    "\n",
    "                state = next_state\n",
    "            \n",
    "                eps_len +=1\n",
    "\n",
    "            print(f\"The {bellman} policy lasted for {eps_len} episodes.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "bins = 15\n",
    "dd = DisDyn(env,bins)\n",
    "\n",
    "bin_limit = 4\n",
    "a = (bins,) * bin_limit\n",
    "\n",
    "for i,s in enumerate(np.ndindex(*a)):\n",
    "    print(f\"s = {s} and my method = {dd.state_to_tuple(i)}  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de635831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
